{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 4959,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002016535591853196,
      "grad_norm": 6.3251142501831055,
      "learning_rate": 4.995462794918331e-05,
      "loss": 0.7747,
      "step": 10
    },
    {
      "epoch": 0.004033071183706392,
      "grad_norm": 3.38026762008667,
      "learning_rate": 4.9904214559386976e-05,
      "loss": 0.6956,
      "step": 20
    },
    {
      "epoch": 0.006049606775559589,
      "grad_norm": 1.4581713676452637,
      "learning_rate": 4.985380116959065e-05,
      "loss": 0.7121,
      "step": 30
    },
    {
      "epoch": 0.008066142367412784,
      "grad_norm": 3.8396778106689453,
      "learning_rate": 4.9803387779794316e-05,
      "loss": 0.6938,
      "step": 40
    },
    {
      "epoch": 0.01008267795926598,
      "grad_norm": 8.102599143981934,
      "learning_rate": 4.975297438999799e-05,
      "loss": 0.6843,
      "step": 50
    },
    {
      "epoch": 0.012099213551119177,
      "grad_norm": 3.1386618614196777,
      "learning_rate": 4.9702561000201656e-05,
      "loss": 0.6753,
      "step": 60
    },
    {
      "epoch": 0.014115749142972374,
      "grad_norm": 15.602036476135254,
      "learning_rate": 4.965214761040532e-05,
      "loss": 0.6601,
      "step": 70
    },
    {
      "epoch": 0.01613228473482557,
      "grad_norm": 17.884126663208008,
      "learning_rate": 4.9601734220608995e-05,
      "loss": 0.6653,
      "step": 80
    },
    {
      "epoch": 0.018148820326678767,
      "grad_norm": 1.0484999418258667,
      "learning_rate": 4.955132083081266e-05,
      "loss": 0.5637,
      "step": 90
    },
    {
      "epoch": 0.02016535591853196,
      "grad_norm": 10.297714233398438,
      "learning_rate": 4.9500907441016335e-05,
      "loss": 0.6394,
      "step": 100
    },
    {
      "epoch": 0.02218189151038516,
      "grad_norm": 5.078429698944092,
      "learning_rate": 4.945049405122001e-05,
      "loss": 0.6289,
      "step": 110
    },
    {
      "epoch": 0.024198427102238355,
      "grad_norm": 2.1044235229492188,
      "learning_rate": 4.9400080661423675e-05,
      "loss": 0.5751,
      "step": 120
    },
    {
      "epoch": 0.02621496269409155,
      "grad_norm": 10.01959228515625,
      "learning_rate": 4.934966727162735e-05,
      "loss": 0.4397,
      "step": 130
    },
    {
      "epoch": 0.028231498285944748,
      "grad_norm": 33.841651916503906,
      "learning_rate": 4.9299253881831015e-05,
      "loss": 0.4612,
      "step": 140
    },
    {
      "epoch": 0.030248033877797943,
      "grad_norm": 10.363471031188965,
      "learning_rate": 4.924884049203469e-05,
      "loss": 0.6569,
      "step": 150
    },
    {
      "epoch": 0.03226456946965114,
      "grad_norm": 23.276077270507812,
      "learning_rate": 4.919842710223836e-05,
      "loss": 0.5483,
      "step": 160
    },
    {
      "epoch": 0.034281105061504336,
      "grad_norm": 6.10534143447876,
      "learning_rate": 4.914801371244203e-05,
      "loss": 0.5988,
      "step": 170
    },
    {
      "epoch": 0.036297640653357534,
      "grad_norm": 9.08410930633545,
      "learning_rate": 4.90976003226457e-05,
      "loss": 0.5823,
      "step": 180
    },
    {
      "epoch": 0.038314176245210725,
      "grad_norm": 12.93310260772705,
      "learning_rate": 4.904718693284937e-05,
      "loss": 0.3702,
      "step": 190
    },
    {
      "epoch": 0.04033071183706392,
      "grad_norm": 28.953378677368164,
      "learning_rate": 4.899677354305304e-05,
      "loss": 0.5899,
      "step": 200
    },
    {
      "epoch": 0.04234724742891712,
      "grad_norm": 79.97979736328125,
      "learning_rate": 4.894636015325671e-05,
      "loss": 0.5596,
      "step": 210
    },
    {
      "epoch": 0.04436378302077032,
      "grad_norm": 48.95529556274414,
      "learning_rate": 4.8895946763460373e-05,
      "loss": 0.5341,
      "step": 220
    },
    {
      "epoch": 0.04638031861262351,
      "grad_norm": 5.7812371253967285,
      "learning_rate": 4.884553337366405e-05,
      "loss": 0.5252,
      "step": 230
    },
    {
      "epoch": 0.04839685420447671,
      "grad_norm": 17.778993606567383,
      "learning_rate": 4.879511998386771e-05,
      "loss": 0.5104,
      "step": 240
    },
    {
      "epoch": 0.05041338979632991,
      "grad_norm": 8.886297225952148,
      "learning_rate": 4.8744706594071386e-05,
      "loss": 0.4412,
      "step": 250
    },
    {
      "epoch": 0.0524299253881831,
      "grad_norm": 3.087899684906006,
      "learning_rate": 4.869429320427506e-05,
      "loss": 0.5104,
      "step": 260
    },
    {
      "epoch": 0.0544464609800363,
      "grad_norm": 8.154070854187012,
      "learning_rate": 4.8643879814478726e-05,
      "loss": 0.6518,
      "step": 270
    },
    {
      "epoch": 0.056462996571889496,
      "grad_norm": 12.808391571044922,
      "learning_rate": 4.85934664246824e-05,
      "loss": 0.4977,
      "step": 280
    },
    {
      "epoch": 0.05847953216374269,
      "grad_norm": 10.34648323059082,
      "learning_rate": 4.8543053034886066e-05,
      "loss": 0.5474,
      "step": 290
    },
    {
      "epoch": 0.060496067755595885,
      "grad_norm": 5.354915142059326,
      "learning_rate": 4.849263964508974e-05,
      "loss": 0.4253,
      "step": 300
    },
    {
      "epoch": 0.06251260334744908,
      "grad_norm": 43.117034912109375,
      "learning_rate": 4.844222625529341e-05,
      "loss": 0.3815,
      "step": 310
    },
    {
      "epoch": 0.06452913893930227,
      "grad_norm": 6.663754940032959,
      "learning_rate": 4.839181286549708e-05,
      "loss": 0.5766,
      "step": 320
    },
    {
      "epoch": 0.06654567453115548,
      "grad_norm": 15.890570640563965,
      "learning_rate": 4.834139947570075e-05,
      "loss": 0.5123,
      "step": 330
    },
    {
      "epoch": 0.06856221012300867,
      "grad_norm": 2.68202543258667,
      "learning_rate": 4.829098608590442e-05,
      "loss": 0.5493,
      "step": 340
    },
    {
      "epoch": 0.07057874571486186,
      "grad_norm": 2.311595916748047,
      "learning_rate": 4.8240572696108085e-05,
      "loss": 0.5995,
      "step": 350
    },
    {
      "epoch": 0.07259528130671507,
      "grad_norm": 3.3711624145507812,
      "learning_rate": 4.819015930631176e-05,
      "loss": 0.2969,
      "step": 360
    },
    {
      "epoch": 0.07461181689856826,
      "grad_norm": 4.193924903869629,
      "learning_rate": 4.8139745916515425e-05,
      "loss": 0.2656,
      "step": 370
    },
    {
      "epoch": 0.07662835249042145,
      "grad_norm": 6.626197814941406,
      "learning_rate": 4.80893325267191e-05,
      "loss": 0.5971,
      "step": 380
    },
    {
      "epoch": 0.07864488808227466,
      "grad_norm": 14.891219139099121,
      "learning_rate": 4.8038919136922765e-05,
      "loss": 0.5721,
      "step": 390
    },
    {
      "epoch": 0.08066142367412785,
      "grad_norm": 32.183258056640625,
      "learning_rate": 4.798850574712644e-05,
      "loss": 0.5321,
      "step": 400
    },
    {
      "epoch": 0.08267795926598104,
      "grad_norm": 4.22998571395874,
      "learning_rate": 4.793809235733011e-05,
      "loss": 0.434,
      "step": 410
    },
    {
      "epoch": 0.08469449485783424,
      "grad_norm": 117.84524536132812,
      "learning_rate": 4.788767896753378e-05,
      "loss": 0.4749,
      "step": 420
    },
    {
      "epoch": 0.08671103044968743,
      "grad_norm": 1.040871262550354,
      "learning_rate": 4.783726557773745e-05,
      "loss": 0.5394,
      "step": 430
    },
    {
      "epoch": 0.08872756604154064,
      "grad_norm": 171.6564483642578,
      "learning_rate": 4.778685218794112e-05,
      "loss": 0.5781,
      "step": 440
    },
    {
      "epoch": 0.09074410163339383,
      "grad_norm": 10.827489852905273,
      "learning_rate": 4.773643879814479e-05,
      "loss": 0.4234,
      "step": 450
    },
    {
      "epoch": 0.09276063722524702,
      "grad_norm": 43.62957000732422,
      "learning_rate": 4.7686025408348464e-05,
      "loss": 0.2572,
      "step": 460
    },
    {
      "epoch": 0.09477717281710023,
      "grad_norm": 99.05931091308594,
      "learning_rate": 4.763561201855213e-05,
      "loss": 0.6714,
      "step": 470
    },
    {
      "epoch": 0.09679370840895342,
      "grad_norm": 2.824101448059082,
      "learning_rate": 4.7585198628755803e-05,
      "loss": 0.5182,
      "step": 480
    },
    {
      "epoch": 0.09881024400080661,
      "grad_norm": 57.57345962524414,
      "learning_rate": 4.753478523895947e-05,
      "loss": 0.5713,
      "step": 490
    },
    {
      "epoch": 0.10082677959265982,
      "grad_norm": 15.936712265014648,
      "learning_rate": 4.7484371849163136e-05,
      "loss": 0.601,
      "step": 500
    },
    {
      "epoch": 0.102843315184513,
      "grad_norm": 20.87114715576172,
      "learning_rate": 4.743395845936681e-05,
      "loss": 0.5809,
      "step": 510
    },
    {
      "epoch": 0.1048598507763662,
      "grad_norm": 6.860533237457275,
      "learning_rate": 4.7383545069570476e-05,
      "loss": 0.4653,
      "step": 520
    },
    {
      "epoch": 0.1068763863682194,
      "grad_norm": 69.15593719482422,
      "learning_rate": 4.733313167977415e-05,
      "loss": 0.4631,
      "step": 530
    },
    {
      "epoch": 0.1088929219600726,
      "grad_norm": 3.426983594894409,
      "learning_rate": 4.7282718289977816e-05,
      "loss": 0.4896,
      "step": 540
    },
    {
      "epoch": 0.11090945755192579,
      "grad_norm": 1.3029762506484985,
      "learning_rate": 4.723230490018149e-05,
      "loss": 0.456,
      "step": 550
    },
    {
      "epoch": 0.11292599314377899,
      "grad_norm": 15.139031410217285,
      "learning_rate": 4.718189151038516e-05,
      "loss": 0.7377,
      "step": 560
    },
    {
      "epoch": 0.11494252873563218,
      "grad_norm": 24.52828598022461,
      "learning_rate": 4.713147812058883e-05,
      "loss": 0.4734,
      "step": 570
    },
    {
      "epoch": 0.11695906432748537,
      "grad_norm": 6.406589031219482,
      "learning_rate": 4.70810647307925e-05,
      "loss": 0.4161,
      "step": 580
    },
    {
      "epoch": 0.11897559991933858,
      "grad_norm": 4.695790767669678,
      "learning_rate": 4.7030651340996175e-05,
      "loss": 0.3652,
      "step": 590
    },
    {
      "epoch": 0.12099213551119177,
      "grad_norm": 2.857243537902832,
      "learning_rate": 4.698023795119984e-05,
      "loss": 0.4005,
      "step": 600
    },
    {
      "epoch": 0.12300867110304498,
      "grad_norm": 7.019220352172852,
      "learning_rate": 4.6929824561403515e-05,
      "loss": 0.4847,
      "step": 610
    },
    {
      "epoch": 0.12502520669489817,
      "grad_norm": 17.522293090820312,
      "learning_rate": 4.687941117160718e-05,
      "loss": 0.5001,
      "step": 620
    },
    {
      "epoch": 0.12704174228675136,
      "grad_norm": 4.138716697692871,
      "learning_rate": 4.6828997781810855e-05,
      "loss": 0.5529,
      "step": 630
    },
    {
      "epoch": 0.12905827787860455,
      "grad_norm": 4.911452293395996,
      "learning_rate": 4.677858439201452e-05,
      "loss": 0.3794,
      "step": 640
    },
    {
      "epoch": 0.13107481347045774,
      "grad_norm": 3.421004295349121,
      "learning_rate": 4.672817100221819e-05,
      "loss": 0.3637,
      "step": 650
    },
    {
      "epoch": 0.13309134906231096,
      "grad_norm": 323.85137939453125,
      "learning_rate": 4.667775761242186e-05,
      "loss": 0.6105,
      "step": 660
    },
    {
      "epoch": 0.13510788465416415,
      "grad_norm": 48.980987548828125,
      "learning_rate": 4.662734422262553e-05,
      "loss": 0.6204,
      "step": 670
    },
    {
      "epoch": 0.13712442024601734,
      "grad_norm": 57.113525390625,
      "learning_rate": 4.65769308328292e-05,
      "loss": 0.4363,
      "step": 680
    },
    {
      "epoch": 0.13914095583787053,
      "grad_norm": 9.41618537902832,
      "learning_rate": 4.6526517443032874e-05,
      "loss": 0.3373,
      "step": 690
    },
    {
      "epoch": 0.14115749142972372,
      "grad_norm": 2.5938291549682617,
      "learning_rate": 4.647610405323654e-05,
      "loss": 0.6711,
      "step": 700
    },
    {
      "epoch": 0.14317402702157694,
      "grad_norm": 17.112342834472656,
      "learning_rate": 4.6425690663440214e-05,
      "loss": 0.4784,
      "step": 710
    },
    {
      "epoch": 0.14519056261343014,
      "grad_norm": 10.95716381072998,
      "learning_rate": 4.637527727364388e-05,
      "loss": 0.3764,
      "step": 720
    },
    {
      "epoch": 0.14720709820528333,
      "grad_norm": 83.88426208496094,
      "learning_rate": 4.6324863883847554e-05,
      "loss": 0.4503,
      "step": 730
    },
    {
      "epoch": 0.14922363379713652,
      "grad_norm": 294.15234375,
      "learning_rate": 4.627445049405123e-05,
      "loss": 0.4576,
      "step": 740
    },
    {
      "epoch": 0.1512401693889897,
      "grad_norm": 24.229412078857422,
      "learning_rate": 4.622403710425489e-05,
      "loss": 0.6936,
      "step": 750
    },
    {
      "epoch": 0.1532567049808429,
      "grad_norm": 15.366830825805664,
      "learning_rate": 4.6173623714458566e-05,
      "loss": 0.4775,
      "step": 760
    },
    {
      "epoch": 0.15527324057269612,
      "grad_norm": 3.398618221282959,
      "learning_rate": 4.612321032466223e-05,
      "loss": 0.5289,
      "step": 770
    },
    {
      "epoch": 0.1572897761645493,
      "grad_norm": 2.3458545207977295,
      "learning_rate": 4.60727969348659e-05,
      "loss": 0.4418,
      "step": 780
    },
    {
      "epoch": 0.1593063117564025,
      "grad_norm": 15.624861717224121,
      "learning_rate": 4.602238354506957e-05,
      "loss": 0.431,
      "step": 790
    },
    {
      "epoch": 0.1613228473482557,
      "grad_norm": 0.8983088731765747,
      "learning_rate": 4.597197015527324e-05,
      "loss": 0.2612,
      "step": 800
    },
    {
      "epoch": 0.16333938294010888,
      "grad_norm": 53.10651779174805,
      "learning_rate": 4.592155676547691e-05,
      "loss": 0.6655,
      "step": 810
    },
    {
      "epoch": 0.16535591853196208,
      "grad_norm": 34.691627502441406,
      "learning_rate": 4.587114337568058e-05,
      "loss": 0.4753,
      "step": 820
    },
    {
      "epoch": 0.1673724541238153,
      "grad_norm": 16.797672271728516,
      "learning_rate": 4.582072998588425e-05,
      "loss": 0.3813,
      "step": 830
    },
    {
      "epoch": 0.1693889897156685,
      "grad_norm": 13.777878761291504,
      "learning_rate": 4.5770316596087925e-05,
      "loss": 0.5755,
      "step": 840
    },
    {
      "epoch": 0.17140552530752168,
      "grad_norm": 18.2769775390625,
      "learning_rate": 4.571990320629159e-05,
      "loss": 0.6581,
      "step": 850
    },
    {
      "epoch": 0.17342206089937487,
      "grad_norm": 2.4180166721343994,
      "learning_rate": 4.5669489816495265e-05,
      "loss": 0.5365,
      "step": 860
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 4.750256061553955,
      "learning_rate": 4.561907642669893e-05,
      "loss": 0.4616,
      "step": 870
    },
    {
      "epoch": 0.17745513208308128,
      "grad_norm": 3.0530710220336914,
      "learning_rate": 4.5568663036902605e-05,
      "loss": 0.5395,
      "step": 880
    },
    {
      "epoch": 0.17947166767493447,
      "grad_norm": 37.737545013427734,
      "learning_rate": 4.551824964710628e-05,
      "loss": 0.4873,
      "step": 890
    },
    {
      "epoch": 0.18148820326678766,
      "grad_norm": 5.227670192718506,
      "learning_rate": 4.5467836257309945e-05,
      "loss": 0.6546,
      "step": 900
    },
    {
      "epoch": 0.18350473885864085,
      "grad_norm": 6.041938304901123,
      "learning_rate": 4.541742286751362e-05,
      "loss": 0.4342,
      "step": 910
    },
    {
      "epoch": 0.18552127445049404,
      "grad_norm": 1.0510876178741455,
      "learning_rate": 4.5367009477717284e-05,
      "loss": 0.4145,
      "step": 920
    },
    {
      "epoch": 0.18753781004234724,
      "grad_norm": 8.763252258300781,
      "learning_rate": 4.531659608792095e-05,
      "loss": 0.7715,
      "step": 930
    },
    {
      "epoch": 0.18955434563420046,
      "grad_norm": 24.877347946166992,
      "learning_rate": 4.5266182698124624e-05,
      "loss": 0.4506,
      "step": 940
    },
    {
      "epoch": 0.19157088122605365,
      "grad_norm": 1.154148817062378,
      "learning_rate": 4.521576930832829e-05,
      "loss": 0.4667,
      "step": 950
    },
    {
      "epoch": 0.19358741681790684,
      "grad_norm": 37.64301300048828,
      "learning_rate": 4.5165355918531964e-05,
      "loss": 0.75,
      "step": 960
    },
    {
      "epoch": 0.19560395240976003,
      "grad_norm": 1.830055594444275,
      "learning_rate": 4.511494252873563e-05,
      "loss": 0.4922,
      "step": 970
    },
    {
      "epoch": 0.19762048800161322,
      "grad_norm": 6.639076232910156,
      "learning_rate": 4.5064529138939304e-05,
      "loss": 0.4809,
      "step": 980
    },
    {
      "epoch": 0.1996370235934664,
      "grad_norm": 3.1117143630981445,
      "learning_rate": 4.501411574914298e-05,
      "loss": 0.4763,
      "step": 990
    },
    {
      "epoch": 0.20165355918531963,
      "grad_norm": 2.7993714809417725,
      "learning_rate": 4.496370235934664e-05,
      "loss": 0.4851,
      "step": 1000
    },
    {
      "epoch": 0.20367009477717282,
      "grad_norm": 1.4614191055297852,
      "learning_rate": 4.4913288969550316e-05,
      "loss": 0.501,
      "step": 1010
    },
    {
      "epoch": 0.205686630369026,
      "grad_norm": 2.029136896133423,
      "learning_rate": 4.486287557975398e-05,
      "loss": 0.4636,
      "step": 1020
    },
    {
      "epoch": 0.2077031659608792,
      "grad_norm": 6.819509506225586,
      "learning_rate": 4.4812462189957656e-05,
      "loss": 0.5679,
      "step": 1030
    },
    {
      "epoch": 0.2097197015527324,
      "grad_norm": 31.158092498779297,
      "learning_rate": 4.476204880016133e-05,
      "loss": 0.3532,
      "step": 1040
    },
    {
      "epoch": 0.21173623714458562,
      "grad_norm": 2.9173922538757324,
      "learning_rate": 4.4711635410364996e-05,
      "loss": 0.6276,
      "step": 1050
    },
    {
      "epoch": 0.2137527727364388,
      "grad_norm": 64.59014129638672,
      "learning_rate": 4.466122202056867e-05,
      "loss": 0.4666,
      "step": 1060
    },
    {
      "epoch": 0.215769308328292,
      "grad_norm": 913.6600341796875,
      "learning_rate": 4.4610808630772336e-05,
      "loss": 0.4756,
      "step": 1070
    },
    {
      "epoch": 0.2177858439201452,
      "grad_norm": 119.31582641601562,
      "learning_rate": 4.4560395240976e-05,
      "loss": 0.4287,
      "step": 1080
    },
    {
      "epoch": 0.21980237951199838,
      "grad_norm": 8.449649810791016,
      "learning_rate": 4.4509981851179675e-05,
      "loss": 0.558,
      "step": 1090
    },
    {
      "epoch": 0.22181891510385157,
      "grad_norm": 3.218233108520508,
      "learning_rate": 4.445956846138334e-05,
      "loss": 0.4005,
      "step": 1100
    },
    {
      "epoch": 0.2238354506957048,
      "grad_norm": 6.687155246734619,
      "learning_rate": 4.4409155071587015e-05,
      "loss": 0.382,
      "step": 1110
    },
    {
      "epoch": 0.22585198628755798,
      "grad_norm": 4.727349281311035,
      "learning_rate": 4.435874168179068e-05,
      "loss": 0.6336,
      "step": 1120
    },
    {
      "epoch": 0.22786852187941117,
      "grad_norm": 25.4143009185791,
      "learning_rate": 4.4308328291994355e-05,
      "loss": 0.554,
      "step": 1130
    },
    {
      "epoch": 0.22988505747126436,
      "grad_norm": 2.1581077575683594,
      "learning_rate": 4.425791490219803e-05,
      "loss": 0.5413,
      "step": 1140
    },
    {
      "epoch": 0.23190159306311756,
      "grad_norm": 4.2266340255737305,
      "learning_rate": 4.4207501512401695e-05,
      "loss": 0.5973,
      "step": 1150
    },
    {
      "epoch": 0.23391812865497075,
      "grad_norm": 23.377227783203125,
      "learning_rate": 4.415708812260537e-05,
      "loss": 0.4858,
      "step": 1160
    },
    {
      "epoch": 0.23593466424682397,
      "grad_norm": 27.9072322845459,
      "learning_rate": 4.4106674732809034e-05,
      "loss": 0.4198,
      "step": 1170
    },
    {
      "epoch": 0.23795119983867716,
      "grad_norm": 2.8596487045288086,
      "learning_rate": 4.405626134301271e-05,
      "loss": 0.3895,
      "step": 1180
    },
    {
      "epoch": 0.23996773543053035,
      "grad_norm": 5.28351354598999,
      "learning_rate": 4.400584795321638e-05,
      "loss": 0.4667,
      "step": 1190
    },
    {
      "epoch": 0.24198427102238354,
      "grad_norm": 5.373502731323242,
      "learning_rate": 4.395543456342005e-05,
      "loss": 0.4034,
      "step": 1200
    },
    {
      "epoch": 0.24400080661423673,
      "grad_norm": 13.935046195983887,
      "learning_rate": 4.3905021173623714e-05,
      "loss": 0.4966,
      "step": 1210
    },
    {
      "epoch": 0.24601734220608995,
      "grad_norm": 10.802862167358398,
      "learning_rate": 4.385460778382739e-05,
      "loss": 0.3989,
      "step": 1220
    },
    {
      "epoch": 0.24803387779794314,
      "grad_norm": 34.460479736328125,
      "learning_rate": 4.3804194394031054e-05,
      "loss": 0.6746,
      "step": 1230
    },
    {
      "epoch": 0.25005041338979633,
      "grad_norm": 2.293374538421631,
      "learning_rate": 4.375378100423473e-05,
      "loss": 0.4423,
      "step": 1240
    },
    {
      "epoch": 0.25206694898164955,
      "grad_norm": 9.914669036865234,
      "learning_rate": 4.370336761443839e-05,
      "loss": 0.4238,
      "step": 1250
    },
    {
      "epoch": 0.2540834845735027,
      "grad_norm": 11.024669647216797,
      "learning_rate": 4.3652954224642067e-05,
      "loss": 0.5289,
      "step": 1260
    },
    {
      "epoch": 0.25610002016535593,
      "grad_norm": 24.991670608520508,
      "learning_rate": 4.360254083484574e-05,
      "loss": 0.5962,
      "step": 1270
    },
    {
      "epoch": 0.2581165557572091,
      "grad_norm": 28.668628692626953,
      "learning_rate": 4.3552127445049406e-05,
      "loss": 0.6532,
      "step": 1280
    },
    {
      "epoch": 0.2601330913490623,
      "grad_norm": 3.9254353046417236,
      "learning_rate": 4.350171405525308e-05,
      "loss": 0.4561,
      "step": 1290
    },
    {
      "epoch": 0.2621496269409155,
      "grad_norm": 5.584336757659912,
      "learning_rate": 4.3451300665456746e-05,
      "loss": 0.3203,
      "step": 1300
    },
    {
      "epoch": 0.2641661625327687,
      "grad_norm": 12.543373107910156,
      "learning_rate": 4.340088727566042e-05,
      "loss": 0.4486,
      "step": 1310
    },
    {
      "epoch": 0.2661826981246219,
      "grad_norm": 88.44658660888672,
      "learning_rate": 4.335047388586409e-05,
      "loss": 0.4909,
      "step": 1320
    },
    {
      "epoch": 0.2681992337164751,
      "grad_norm": 2.998595952987671,
      "learning_rate": 4.330006049606776e-05,
      "loss": 0.6075,
      "step": 1330
    },
    {
      "epoch": 0.2702157693083283,
      "grad_norm": 64.92391204833984,
      "learning_rate": 4.324964710627143e-05,
      "loss": 0.3269,
      "step": 1340
    },
    {
      "epoch": 0.27223230490018147,
      "grad_norm": 16.466598510742188,
      "learning_rate": 4.31992337164751e-05,
      "loss": 0.4075,
      "step": 1350
    },
    {
      "epoch": 0.2742488404920347,
      "grad_norm": 26.009033203125,
      "learning_rate": 4.3148820326678765e-05,
      "loss": 0.48,
      "step": 1360
    },
    {
      "epoch": 0.2762653760838879,
      "grad_norm": 15.461030006408691,
      "learning_rate": 4.309840693688244e-05,
      "loss": 0.473,
      "step": 1370
    },
    {
      "epoch": 0.27828191167574107,
      "grad_norm": 2.0539538860321045,
      "learning_rate": 4.3047993547086105e-05,
      "loss": 0.4754,
      "step": 1380
    },
    {
      "epoch": 0.2802984472675943,
      "grad_norm": 2.4636526107788086,
      "learning_rate": 4.299758015728978e-05,
      "loss": 0.3933,
      "step": 1390
    },
    {
      "epoch": 0.28231498285944745,
      "grad_norm": 1.3880057334899902,
      "learning_rate": 4.2947166767493445e-05,
      "loss": 0.3988,
      "step": 1400
    },
    {
      "epoch": 0.28433151845130067,
      "grad_norm": 14.329094886779785,
      "learning_rate": 4.289675337769712e-05,
      "loss": 0.4005,
      "step": 1410
    },
    {
      "epoch": 0.2863480540431539,
      "grad_norm": 8.46279239654541,
      "learning_rate": 4.284633998790079e-05,
      "loss": 0.6451,
      "step": 1420
    },
    {
      "epoch": 0.28836458963500705,
      "grad_norm": 3.428652286529541,
      "learning_rate": 4.279592659810446e-05,
      "loss": 0.3545,
      "step": 1430
    },
    {
      "epoch": 0.29038112522686027,
      "grad_norm": 1.0026053190231323,
      "learning_rate": 4.274551320830813e-05,
      "loss": 0.442,
      "step": 1440
    },
    {
      "epoch": 0.29239766081871343,
      "grad_norm": 8.371488571166992,
      "learning_rate": 4.26950998185118e-05,
      "loss": 0.4344,
      "step": 1450
    },
    {
      "epoch": 0.29441419641056665,
      "grad_norm": 1.5638272762298584,
      "learning_rate": 4.264468642871547e-05,
      "loss": 0.3671,
      "step": 1460
    },
    {
      "epoch": 0.2964307320024198,
      "grad_norm": 2.3419878482818604,
      "learning_rate": 4.2594273038919144e-05,
      "loss": 0.4794,
      "step": 1470
    },
    {
      "epoch": 0.29844726759427304,
      "grad_norm": 35.777183532714844,
      "learning_rate": 4.254385964912281e-05,
      "loss": 0.455,
      "step": 1480
    },
    {
      "epoch": 0.30046380318612625,
      "grad_norm": 15.461639404296875,
      "learning_rate": 4.2493446259326484e-05,
      "loss": 0.5237,
      "step": 1490
    },
    {
      "epoch": 0.3024803387779794,
      "grad_norm": 1.1361567974090576,
      "learning_rate": 4.244303286953015e-05,
      "loss": 0.4982,
      "step": 1500
    },
    {
      "epoch": 0.30449687436983264,
      "grad_norm": 4.3481903076171875,
      "learning_rate": 4.2392619479733817e-05,
      "loss": 0.3982,
      "step": 1510
    },
    {
      "epoch": 0.3065134099616858,
      "grad_norm": 8.21597671508789,
      "learning_rate": 4.234220608993749e-05,
      "loss": 0.504,
      "step": 1520
    },
    {
      "epoch": 0.308529945553539,
      "grad_norm": 5.3722243309021,
      "learning_rate": 4.2291792700141156e-05,
      "loss": 0.4432,
      "step": 1530
    },
    {
      "epoch": 0.31054648114539224,
      "grad_norm": 19.8753662109375,
      "learning_rate": 4.224137931034483e-05,
      "loss": 0.3395,
      "step": 1540
    },
    {
      "epoch": 0.3125630167372454,
      "grad_norm": 4.054671764373779,
      "learning_rate": 4.2190965920548496e-05,
      "loss": 0.2947,
      "step": 1550
    },
    {
      "epoch": 0.3145795523290986,
      "grad_norm": 2.2235262393951416,
      "learning_rate": 4.214055253075217e-05,
      "loss": 0.362,
      "step": 1560
    },
    {
      "epoch": 0.3165960879209518,
      "grad_norm": 17.058658599853516,
      "learning_rate": 4.209013914095584e-05,
      "loss": 0.2832,
      "step": 1570
    },
    {
      "epoch": 0.318612623512805,
      "grad_norm": 16.9038143157959,
      "learning_rate": 4.203972575115951e-05,
      "loss": 0.4332,
      "step": 1580
    },
    {
      "epoch": 0.3206291591046582,
      "grad_norm": 20.07639503479004,
      "learning_rate": 4.198931236136318e-05,
      "loss": 0.6299,
      "step": 1590
    },
    {
      "epoch": 0.3226456946965114,
      "grad_norm": 1.18110990524292,
      "learning_rate": 4.193889897156685e-05,
      "loss": 0.3437,
      "step": 1600
    },
    {
      "epoch": 0.3246622302883646,
      "grad_norm": 5.231297016143799,
      "learning_rate": 4.188848558177052e-05,
      "loss": 0.3893,
      "step": 1610
    },
    {
      "epoch": 0.32667876588021777,
      "grad_norm": 21.541269302368164,
      "learning_rate": 4.1838072191974195e-05,
      "loss": 0.4539,
      "step": 1620
    },
    {
      "epoch": 0.328695301472071,
      "grad_norm": 7.009695053100586,
      "learning_rate": 4.178765880217786e-05,
      "loss": 0.685,
      "step": 1630
    },
    {
      "epoch": 0.33071183706392415,
      "grad_norm": 9.201099395751953,
      "learning_rate": 4.173724541238153e-05,
      "loss": 0.3396,
      "step": 1640
    },
    {
      "epoch": 0.33272837265577737,
      "grad_norm": 41.887271881103516,
      "learning_rate": 4.16868320225852e-05,
      "loss": 0.5418,
      "step": 1650
    },
    {
      "epoch": 0.3347449082476306,
      "grad_norm": 8.581245422363281,
      "learning_rate": 4.163641863278887e-05,
      "loss": 0.5046,
      "step": 1660
    },
    {
      "epoch": 0.33676144383948375,
      "grad_norm": 2.1884119510650635,
      "learning_rate": 4.158600524299254e-05,
      "loss": 0.3964,
      "step": 1670
    },
    {
      "epoch": 0.338777979431337,
      "grad_norm": 13.642645835876465,
      "learning_rate": 4.153559185319621e-05,
      "loss": 0.472,
      "step": 1680
    },
    {
      "epoch": 0.34079451502319014,
      "grad_norm": 0.9501444697380066,
      "learning_rate": 4.148517846339988e-05,
      "loss": 0.4514,
      "step": 1690
    },
    {
      "epoch": 0.34281105061504336,
      "grad_norm": 8.458125114440918,
      "learning_rate": 4.143476507360355e-05,
      "loss": 0.5004,
      "step": 1700
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 89.03797912597656,
      "learning_rate": 4.138435168380722e-05,
      "loss": 0.4781,
      "step": 1710
    },
    {
      "epoch": 0.34684412179874974,
      "grad_norm": 10.870992660522461,
      "learning_rate": 4.1333938294010894e-05,
      "loss": 0.4549,
      "step": 1720
    },
    {
      "epoch": 0.34886065739060296,
      "grad_norm": 18.687591552734375,
      "learning_rate": 4.128352490421456e-05,
      "loss": 0.3608,
      "step": 1730
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 10.046449661254883,
      "learning_rate": 4.1233111514418234e-05,
      "loss": 0.454,
      "step": 1740
    },
    {
      "epoch": 0.35289372857430934,
      "grad_norm": 1.7334016561508179,
      "learning_rate": 4.11826981246219e-05,
      "loss": 0.3418,
      "step": 1750
    },
    {
      "epoch": 0.35491026416616256,
      "grad_norm": 25.669559478759766,
      "learning_rate": 4.113228473482557e-05,
      "loss": 0.5377,
      "step": 1760
    },
    {
      "epoch": 0.3569267997580157,
      "grad_norm": 10.520331382751465,
      "learning_rate": 4.1081871345029247e-05,
      "loss": 0.5483,
      "step": 1770
    },
    {
      "epoch": 0.35894333534986894,
      "grad_norm": 9.744111061096191,
      "learning_rate": 4.103145795523291e-05,
      "loss": 0.4328,
      "step": 1780
    },
    {
      "epoch": 0.3609598709417221,
      "grad_norm": 14.984160423278809,
      "learning_rate": 4.098104456543658e-05,
      "loss": 0.2951,
      "step": 1790
    },
    {
      "epoch": 0.3629764065335753,
      "grad_norm": 4.485157489776611,
      "learning_rate": 4.093063117564025e-05,
      "loss": 0.5819,
      "step": 1800
    },
    {
      "epoch": 0.3649929421254285,
      "grad_norm": 7.199477195739746,
      "learning_rate": 4.088021778584392e-05,
      "loss": 0.6501,
      "step": 1810
    },
    {
      "epoch": 0.3670094777172817,
      "grad_norm": 5.267457485198975,
      "learning_rate": 4.082980439604759e-05,
      "loss": 0.5246,
      "step": 1820
    },
    {
      "epoch": 0.3690260133091349,
      "grad_norm": 1.4832630157470703,
      "learning_rate": 4.077939100625126e-05,
      "loss": 0.3339,
      "step": 1830
    },
    {
      "epoch": 0.3710425489009881,
      "grad_norm": 16.284685134887695,
      "learning_rate": 4.072897761645493e-05,
      "loss": 0.5029,
      "step": 1840
    },
    {
      "epoch": 0.3730590844928413,
      "grad_norm": 0.6094575524330139,
      "learning_rate": 4.06785642266586e-05,
      "loss": 0.2967,
      "step": 1850
    },
    {
      "epoch": 0.37507562008469447,
      "grad_norm": 2.7400195598602295,
      "learning_rate": 4.062815083686227e-05,
      "loss": 0.8018,
      "step": 1860
    },
    {
      "epoch": 0.3770921556765477,
      "grad_norm": 38.82992935180664,
      "learning_rate": 4.0577737447065945e-05,
      "loss": 0.4231,
      "step": 1870
    },
    {
      "epoch": 0.3791086912684009,
      "grad_norm": 1.9953519105911255,
      "learning_rate": 4.052732405726961e-05,
      "loss": 0.3829,
      "step": 1880
    },
    {
      "epoch": 0.3811252268602541,
      "grad_norm": 4.844915866851807,
      "learning_rate": 4.0476910667473285e-05,
      "loss": 0.5126,
      "step": 1890
    },
    {
      "epoch": 0.3831417624521073,
      "grad_norm": 6.72674560546875,
      "learning_rate": 4.042649727767695e-05,
      "loss": 0.4193,
      "step": 1900
    },
    {
      "epoch": 0.38515829804396046,
      "grad_norm": 64.90362548828125,
      "learning_rate": 4.0376083887880625e-05,
      "loss": 0.4043,
      "step": 1910
    },
    {
      "epoch": 0.3871748336358137,
      "grad_norm": 5.074402809143066,
      "learning_rate": 4.03256704980843e-05,
      "loss": 0.2573,
      "step": 1920
    },
    {
      "epoch": 0.3891913692276669,
      "grad_norm": 28.936710357666016,
      "learning_rate": 4.0275257108287964e-05,
      "loss": 0.3753,
      "step": 1930
    },
    {
      "epoch": 0.39120790481952006,
      "grad_norm": 0.6120311617851257,
      "learning_rate": 4.022484371849163e-05,
      "loss": 0.2521,
      "step": 1940
    },
    {
      "epoch": 0.3932244404113733,
      "grad_norm": 49.1609001159668,
      "learning_rate": 4.0174430328695304e-05,
      "loss": 0.5211,
      "step": 1950
    },
    {
      "epoch": 0.39524097600322644,
      "grad_norm": 1.3137930631637573,
      "learning_rate": 4.012401693889897e-05,
      "loss": 0.3206,
      "step": 1960
    },
    {
      "epoch": 0.39725751159507966,
      "grad_norm": 6.462067127227783,
      "learning_rate": 4.0073603549102644e-05,
      "loss": 0.4101,
      "step": 1970
    },
    {
      "epoch": 0.3992740471869328,
      "grad_norm": 66.16756439208984,
      "learning_rate": 4.002319015930631e-05,
      "loss": 0.3411,
      "step": 1980
    },
    {
      "epoch": 0.40129058277878604,
      "grad_norm": 19.80302619934082,
      "learning_rate": 3.9972776769509984e-05,
      "loss": 0.4435,
      "step": 1990
    },
    {
      "epoch": 0.40330711837063926,
      "grad_norm": 2.7159650325775146,
      "learning_rate": 3.992236337971366e-05,
      "loss": 0.5018,
      "step": 2000
    },
    {
      "epoch": 0.4053236539624924,
      "grad_norm": 2.265105962753296,
      "learning_rate": 3.987194998991732e-05,
      "loss": 0.6625,
      "step": 2010
    },
    {
      "epoch": 0.40734018955434564,
      "grad_norm": 5.443859100341797,
      "learning_rate": 3.9821536600120997e-05,
      "loss": 0.4884,
      "step": 2020
    },
    {
      "epoch": 0.4093567251461988,
      "grad_norm": 6.893690586090088,
      "learning_rate": 3.977112321032466e-05,
      "loss": 0.3746,
      "step": 2030
    },
    {
      "epoch": 0.411373260738052,
      "grad_norm": 1.9668190479278564,
      "learning_rate": 3.9720709820528336e-05,
      "loss": 0.4108,
      "step": 2040
    },
    {
      "epoch": 0.41338979632990525,
      "grad_norm": 51.9446907043457,
      "learning_rate": 3.967029643073201e-05,
      "loss": 0.5906,
      "step": 2050
    },
    {
      "epoch": 0.4154063319217584,
      "grad_norm": 1.497475028038025,
      "learning_rate": 3.9619883040935676e-05,
      "loss": 0.3855,
      "step": 2060
    },
    {
      "epoch": 0.41742286751361163,
      "grad_norm": 11.024585723876953,
      "learning_rate": 3.956946965113935e-05,
      "loss": 0.3762,
      "step": 2070
    },
    {
      "epoch": 0.4194394031054648,
      "grad_norm": 49.5217170715332,
      "learning_rate": 3.9519056261343016e-05,
      "loss": 0.271,
      "step": 2080
    },
    {
      "epoch": 0.421455938697318,
      "grad_norm": 28.671188354492188,
      "learning_rate": 3.946864287154668e-05,
      "loss": 0.4579,
      "step": 2090
    },
    {
      "epoch": 0.42347247428917123,
      "grad_norm": 5.4868597984313965,
      "learning_rate": 3.9418229481750355e-05,
      "loss": 0.5151,
      "step": 2100
    },
    {
      "epoch": 0.4254890098810244,
      "grad_norm": 18.534650802612305,
      "learning_rate": 3.936781609195402e-05,
      "loss": 0.3342,
      "step": 2110
    },
    {
      "epoch": 0.4275055454728776,
      "grad_norm": 5.700922012329102,
      "learning_rate": 3.9317402702157695e-05,
      "loss": 0.3992,
      "step": 2120
    },
    {
      "epoch": 0.4295220810647308,
      "grad_norm": 3.594521999359131,
      "learning_rate": 3.926698931236136e-05,
      "loss": 0.4706,
      "step": 2130
    },
    {
      "epoch": 0.431538616656584,
      "grad_norm": 3.5803728103637695,
      "learning_rate": 3.9216575922565035e-05,
      "loss": 0.3665,
      "step": 2140
    },
    {
      "epoch": 0.43355515224843716,
      "grad_norm": 2.8562467098236084,
      "learning_rate": 3.916616253276871e-05,
      "loss": 0.4391,
      "step": 2150
    },
    {
      "epoch": 0.4355716878402904,
      "grad_norm": 1.2840138673782349,
      "learning_rate": 3.9115749142972375e-05,
      "loss": 0.4049,
      "step": 2160
    },
    {
      "epoch": 0.4375882234321436,
      "grad_norm": 3.2650842666625977,
      "learning_rate": 3.906533575317605e-05,
      "loss": 0.369,
      "step": 2170
    },
    {
      "epoch": 0.43960475902399676,
      "grad_norm": 53.37433624267578,
      "learning_rate": 3.9014922363379714e-05,
      "loss": 0.2682,
      "step": 2180
    },
    {
      "epoch": 0.44162129461585,
      "grad_norm": 15.493186950683594,
      "learning_rate": 3.896450897358339e-05,
      "loss": 0.2506,
      "step": 2190
    },
    {
      "epoch": 0.44363783020770314,
      "grad_norm": 2.4735114574432373,
      "learning_rate": 3.891409558378706e-05,
      "loss": 0.4199,
      "step": 2200
    },
    {
      "epoch": 0.44565436579955636,
      "grad_norm": 8.908304214477539,
      "learning_rate": 3.886368219399073e-05,
      "loss": 0.6762,
      "step": 2210
    },
    {
      "epoch": 0.4476709013914096,
      "grad_norm": 5.515600204467773,
      "learning_rate": 3.8813268804194394e-05,
      "loss": 0.4666,
      "step": 2220
    },
    {
      "epoch": 0.44968743698326274,
      "grad_norm": 3.4795212745666504,
      "learning_rate": 3.876285541439807e-05,
      "loss": 0.4109,
      "step": 2230
    },
    {
      "epoch": 0.45170397257511596,
      "grad_norm": 20.214555740356445,
      "learning_rate": 3.8712442024601734e-05,
      "loss": 0.4896,
      "step": 2240
    },
    {
      "epoch": 0.4537205081669691,
      "grad_norm": 7.0811967849731445,
      "learning_rate": 3.866202863480541e-05,
      "loss": 0.4929,
      "step": 2250
    },
    {
      "epoch": 0.45573704375882235,
      "grad_norm": 3.0693862438201904,
      "learning_rate": 3.861161524500907e-05,
      "loss": 0.4406,
      "step": 2260
    },
    {
      "epoch": 0.45775357935067557,
      "grad_norm": 9.070863723754883,
      "learning_rate": 3.8561201855212747e-05,
      "loss": 0.3965,
      "step": 2270
    },
    {
      "epoch": 0.45977011494252873,
      "grad_norm": 1.5101827383041382,
      "learning_rate": 3.851078846541641e-05,
      "loss": 0.3208,
      "step": 2280
    },
    {
      "epoch": 0.46178665053438195,
      "grad_norm": 2.320906162261963,
      "learning_rate": 3.8460375075620086e-05,
      "loss": 0.4386,
      "step": 2290
    },
    {
      "epoch": 0.4638031861262351,
      "grad_norm": 6.660170078277588,
      "learning_rate": 3.840996168582376e-05,
      "loss": 0.4048,
      "step": 2300
    },
    {
      "epoch": 0.46581972171808833,
      "grad_norm": 6.125809192657471,
      "learning_rate": 3.8359548296027426e-05,
      "loss": 0.2768,
      "step": 2310
    },
    {
      "epoch": 0.4678362573099415,
      "grad_norm": 3.5966784954071045,
      "learning_rate": 3.83091349062311e-05,
      "loss": 0.4789,
      "step": 2320
    },
    {
      "epoch": 0.4698527929017947,
      "grad_norm": 28.034576416015625,
      "learning_rate": 3.8258721516434766e-05,
      "loss": 0.5096,
      "step": 2330
    },
    {
      "epoch": 0.47186932849364793,
      "grad_norm": 3.1466729640960693,
      "learning_rate": 3.820830812663844e-05,
      "loss": 0.5341,
      "step": 2340
    },
    {
      "epoch": 0.4738858640855011,
      "grad_norm": 1.7981746196746826,
      "learning_rate": 3.815789473684211e-05,
      "loss": 0.3528,
      "step": 2350
    },
    {
      "epoch": 0.4759023996773543,
      "grad_norm": 2.2884998321533203,
      "learning_rate": 3.810748134704578e-05,
      "loss": 0.4444,
      "step": 2360
    },
    {
      "epoch": 0.4779189352692075,
      "grad_norm": 83.65898132324219,
      "learning_rate": 3.8057067957249445e-05,
      "loss": 0.4006,
      "step": 2370
    },
    {
      "epoch": 0.4799354708610607,
      "grad_norm": 2.235231876373291,
      "learning_rate": 3.800665456745311e-05,
      "loss": 0.4811,
      "step": 2380
    },
    {
      "epoch": 0.4819520064529139,
      "grad_norm": 26.553314208984375,
      "learning_rate": 3.7956241177656785e-05,
      "loss": 0.3389,
      "step": 2390
    },
    {
      "epoch": 0.4839685420447671,
      "grad_norm": 18.576398849487305,
      "learning_rate": 3.790582778786046e-05,
      "loss": 0.241,
      "step": 2400
    },
    {
      "epoch": 0.4859850776366203,
      "grad_norm": 5.197652339935303,
      "learning_rate": 3.7855414398064125e-05,
      "loss": 0.5346,
      "step": 2410
    },
    {
      "epoch": 0.48800161322847346,
      "grad_norm": 78.6335220336914,
      "learning_rate": 3.78050010082678e-05,
      "loss": 0.5029,
      "step": 2420
    },
    {
      "epoch": 0.4900181488203267,
      "grad_norm": 18.544422149658203,
      "learning_rate": 3.7754587618471464e-05,
      "loss": 0.4256,
      "step": 2430
    },
    {
      "epoch": 0.4920346844121799,
      "grad_norm": 1.1685055494308472,
      "learning_rate": 3.770417422867514e-05,
      "loss": 0.3558,
      "step": 2440
    },
    {
      "epoch": 0.49405122000403306,
      "grad_norm": 2.541379928588867,
      "learning_rate": 3.765376083887881e-05,
      "loss": 0.4863,
      "step": 2450
    },
    {
      "epoch": 0.4960677555958863,
      "grad_norm": 20.007572174072266,
      "learning_rate": 3.760334744908248e-05,
      "loss": 0.4206,
      "step": 2460
    },
    {
      "epoch": 0.49808429118773945,
      "grad_norm": 133.8644561767578,
      "learning_rate": 3.755293405928615e-05,
      "loss": 0.5243,
      "step": 2470
    },
    {
      "epoch": 0.5001008267795927,
      "grad_norm": 11.472935676574707,
      "learning_rate": 3.750252066948982e-05,
      "loss": 0.3189,
      "step": 2480
    },
    {
      "epoch": 0.5021173623714459,
      "grad_norm": 2.0812811851501465,
      "learning_rate": 3.745210727969349e-05,
      "loss": 0.2735,
      "step": 2490
    },
    {
      "epoch": 0.5041338979632991,
      "grad_norm": 31.7398738861084,
      "learning_rate": 3.7401693889897164e-05,
      "loss": 0.3867,
      "step": 2500
    },
    {
      "epoch": 0.5061504335551522,
      "grad_norm": 6.707540988922119,
      "learning_rate": 3.735128050010083e-05,
      "loss": 0.4212,
      "step": 2510
    },
    {
      "epoch": 0.5081669691470054,
      "grad_norm": 1.3248884677886963,
      "learning_rate": 3.7300867110304497e-05,
      "loss": 0.5028,
      "step": 2520
    },
    {
      "epoch": 0.5101835047388587,
      "grad_norm": 4.452311992645264,
      "learning_rate": 3.725045372050816e-05,
      "loss": 0.5189,
      "step": 2530
    },
    {
      "epoch": 0.5122000403307119,
      "grad_norm": 3.3496809005737305,
      "learning_rate": 3.7200040330711836e-05,
      "loss": 0.4199,
      "step": 2540
    },
    {
      "epoch": 0.514216575922565,
      "grad_norm": 1.530256748199463,
      "learning_rate": 3.714962694091551e-05,
      "loss": 0.3591,
      "step": 2550
    },
    {
      "epoch": 0.5162331115144182,
      "grad_norm": 2.1021437644958496,
      "learning_rate": 3.7099213551119176e-05,
      "loss": 0.3933,
      "step": 2560
    },
    {
      "epoch": 0.5182496471062714,
      "grad_norm": 0.7409253120422363,
      "learning_rate": 3.704880016132285e-05,
      "loss": 0.4326,
      "step": 2570
    },
    {
      "epoch": 0.5202661826981246,
      "grad_norm": 23.015026092529297,
      "learning_rate": 3.6998386771526516e-05,
      "loss": 0.3963,
      "step": 2580
    },
    {
      "epoch": 0.5222827182899779,
      "grad_norm": 10.620753288269043,
      "learning_rate": 3.694797338173019e-05,
      "loss": 0.5518,
      "step": 2590
    },
    {
      "epoch": 0.524299253881831,
      "grad_norm": 150.16041564941406,
      "learning_rate": 3.689755999193386e-05,
      "loss": 0.5655,
      "step": 2600
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 1.2485624551773071,
      "learning_rate": 3.684714660213753e-05,
      "loss": 0.2898,
      "step": 2610
    },
    {
      "epoch": 0.5283323250655374,
      "grad_norm": 4.767935276031494,
      "learning_rate": 3.67967332123412e-05,
      "loss": 0.5975,
      "step": 2620
    },
    {
      "epoch": 0.5303488606573906,
      "grad_norm": 84.25980377197266,
      "learning_rate": 3.6746319822544875e-05,
      "loss": 0.5561,
      "step": 2630
    },
    {
      "epoch": 0.5323653962492438,
      "grad_norm": 3.532393217086792,
      "learning_rate": 3.669590643274854e-05,
      "loss": 0.3589,
      "step": 2640
    },
    {
      "epoch": 0.534381931841097,
      "grad_norm": 15.6353120803833,
      "learning_rate": 3.664549304295221e-05,
      "loss": 0.2622,
      "step": 2650
    },
    {
      "epoch": 0.5363984674329502,
      "grad_norm": 35.31687545776367,
      "learning_rate": 3.659507965315588e-05,
      "loss": 0.3289,
      "step": 2660
    },
    {
      "epoch": 0.5384150030248034,
      "grad_norm": 1.7421746253967285,
      "learning_rate": 3.654466626335955e-05,
      "loss": 0.2635,
      "step": 2670
    },
    {
      "epoch": 0.5404315386166566,
      "grad_norm": 3.3342981338500977,
      "learning_rate": 3.649425287356322e-05,
      "loss": 0.5499,
      "step": 2680
    },
    {
      "epoch": 0.5424480742085098,
      "grad_norm": 1.8787602186203003,
      "learning_rate": 3.644383948376689e-05,
      "loss": 0.4929,
      "step": 2690
    },
    {
      "epoch": 0.5444646098003629,
      "grad_norm": 0.6427419185638428,
      "learning_rate": 3.639342609397056e-05,
      "loss": 0.4066,
      "step": 2700
    },
    {
      "epoch": 0.5464811453922161,
      "grad_norm": 1.8744187355041504,
      "learning_rate": 3.634301270417423e-05,
      "loss": 0.2913,
      "step": 2710
    },
    {
      "epoch": 0.5484976809840694,
      "grad_norm": 2.263786792755127,
      "learning_rate": 3.62925993143779e-05,
      "loss": 0.4764,
      "step": 2720
    },
    {
      "epoch": 0.5505142165759226,
      "grad_norm": 1.4821442365646362,
      "learning_rate": 3.6242185924581574e-05,
      "loss": 0.4588,
      "step": 2730
    },
    {
      "epoch": 0.5525307521677758,
      "grad_norm": 2.323385238647461,
      "learning_rate": 3.619177253478524e-05,
      "loss": 0.4406,
      "step": 2740
    },
    {
      "epoch": 0.5545472877596289,
      "grad_norm": 13.651862144470215,
      "learning_rate": 3.6141359144988914e-05,
      "loss": 0.3957,
      "step": 2750
    },
    {
      "epoch": 0.5565638233514821,
      "grad_norm": 7.042654991149902,
      "learning_rate": 3.609094575519258e-05,
      "loss": 0.3529,
      "step": 2760
    },
    {
      "epoch": 0.5585803589433354,
      "grad_norm": 0.8278635144233704,
      "learning_rate": 3.604053236539625e-05,
      "loss": 0.364,
      "step": 2770
    },
    {
      "epoch": 0.5605968945351886,
      "grad_norm": 6.926609992980957,
      "learning_rate": 3.599011897559993e-05,
      "loss": 0.424,
      "step": 2780
    },
    {
      "epoch": 0.5626134301270418,
      "grad_norm": 37.56672286987305,
      "learning_rate": 3.593970558580359e-05,
      "loss": 0.2766,
      "step": 2790
    },
    {
      "epoch": 0.5646299657188949,
      "grad_norm": 19.444351196289062,
      "learning_rate": 3.588929219600726e-05,
      "loss": 0.5918,
      "step": 2800
    },
    {
      "epoch": 0.5666465013107481,
      "grad_norm": 1.5324490070343018,
      "learning_rate": 3.5838878806210926e-05,
      "loss": 0.5514,
      "step": 2810
    },
    {
      "epoch": 0.5686630369026013,
      "grad_norm": 3.873739242553711,
      "learning_rate": 3.57884654164146e-05,
      "loss": 0.4118,
      "step": 2820
    },
    {
      "epoch": 0.5706795724944546,
      "grad_norm": 27.348297119140625,
      "learning_rate": 3.573805202661827e-05,
      "loss": 0.4084,
      "step": 2830
    },
    {
      "epoch": 0.5726961080863078,
      "grad_norm": 4.256318092346191,
      "learning_rate": 3.568763863682194e-05,
      "loss": 0.443,
      "step": 2840
    },
    {
      "epoch": 0.5747126436781609,
      "grad_norm": 11.014415740966797,
      "learning_rate": 3.563722524702561e-05,
      "loss": 0.3379,
      "step": 2850
    },
    {
      "epoch": 0.5767291792700141,
      "grad_norm": 58.10097122192383,
      "learning_rate": 3.558681185722928e-05,
      "loss": 0.3814,
      "step": 2860
    },
    {
      "epoch": 0.5787457148618673,
      "grad_norm": 2.428056478500366,
      "learning_rate": 3.553639846743295e-05,
      "loss": 0.3583,
      "step": 2870
    },
    {
      "epoch": 0.5807622504537205,
      "grad_norm": 11.999417304992676,
      "learning_rate": 3.5485985077636625e-05,
      "loss": 0.5034,
      "step": 2880
    },
    {
      "epoch": 0.5827787860455736,
      "grad_norm": 10.633814811706543,
      "learning_rate": 3.543557168784029e-05,
      "loss": 0.5274,
      "step": 2890
    },
    {
      "epoch": 0.5847953216374269,
      "grad_norm": 1.889489769935608,
      "learning_rate": 3.5385158298043965e-05,
      "loss": 0.283,
      "step": 2900
    },
    {
      "epoch": 0.5868118572292801,
      "grad_norm": 11.582491874694824,
      "learning_rate": 3.533474490824763e-05,
      "loss": 0.3576,
      "step": 2910
    },
    {
      "epoch": 0.5888283928211333,
      "grad_norm": 74.70707702636719,
      "learning_rate": 3.5284331518451305e-05,
      "loss": 0.3989,
      "step": 2920
    },
    {
      "epoch": 0.5908449284129865,
      "grad_norm": 0.6735146045684814,
      "learning_rate": 3.523391812865498e-05,
      "loss": 0.5637,
      "step": 2930
    },
    {
      "epoch": 0.5928614640048396,
      "grad_norm": 1.1953072547912598,
      "learning_rate": 3.5183504738858644e-05,
      "loss": 0.4998,
      "step": 2940
    },
    {
      "epoch": 0.5948779995966929,
      "grad_norm": 1.6057780981063843,
      "learning_rate": 3.513309134906231e-05,
      "loss": 0.3721,
      "step": 2950
    },
    {
      "epoch": 0.5968945351885461,
      "grad_norm": 3.1911275386810303,
      "learning_rate": 3.508267795926598e-05,
      "loss": 0.5008,
      "step": 2960
    },
    {
      "epoch": 0.5989110707803993,
      "grad_norm": 12.22616958618164,
      "learning_rate": 3.503226456946965e-05,
      "loss": 0.4707,
      "step": 2970
    },
    {
      "epoch": 0.6009276063722525,
      "grad_norm": 1.3646332025527954,
      "learning_rate": 3.4981851179673324e-05,
      "loss": 0.4391,
      "step": 2980
    },
    {
      "epoch": 0.6029441419641056,
      "grad_norm": 3.6762025356292725,
      "learning_rate": 3.493143778987699e-05,
      "loss": 0.3547,
      "step": 2990
    },
    {
      "epoch": 0.6049606775559588,
      "grad_norm": 1.1792908906936646,
      "learning_rate": 3.4881024400080664e-05,
      "loss": 0.3419,
      "step": 3000
    },
    {
      "epoch": 0.6069772131478121,
      "grad_norm": 7.872889995574951,
      "learning_rate": 3.483061101028433e-05,
      "loss": 0.3933,
      "step": 3010
    },
    {
      "epoch": 0.6089937487396653,
      "grad_norm": 1.1725422143936157,
      "learning_rate": 3.4780197620488e-05,
      "loss": 0.3791,
      "step": 3020
    },
    {
      "epoch": 0.6110102843315185,
      "grad_norm": 1.1896530389785767,
      "learning_rate": 3.472978423069168e-05,
      "loss": 0.2866,
      "step": 3030
    },
    {
      "epoch": 0.6130268199233716,
      "grad_norm": 66.7421646118164,
      "learning_rate": 3.467937084089534e-05,
      "loss": 0.304,
      "step": 3040
    },
    {
      "epoch": 0.6150433555152248,
      "grad_norm": 4.552791118621826,
      "learning_rate": 3.4628957451099016e-05,
      "loss": 0.5293,
      "step": 3050
    },
    {
      "epoch": 0.617059891107078,
      "grad_norm": 3.9856863021850586,
      "learning_rate": 3.457854406130268e-05,
      "loss": 0.6033,
      "step": 3060
    },
    {
      "epoch": 0.6190764266989313,
      "grad_norm": 2.141199827194214,
      "learning_rate": 3.4528130671506356e-05,
      "loss": 0.3466,
      "step": 3070
    },
    {
      "epoch": 0.6210929622907845,
      "grad_norm": 5.959115505218506,
      "learning_rate": 3.447771728171002e-05,
      "loss": 0.5565,
      "step": 3080
    },
    {
      "epoch": 0.6231094978826376,
      "grad_norm": 1.2630616426467896,
      "learning_rate": 3.4427303891913696e-05,
      "loss": 0.2703,
      "step": 3090
    },
    {
      "epoch": 0.6251260334744908,
      "grad_norm": 5.5470356941223145,
      "learning_rate": 3.437689050211736e-05,
      "loss": 0.5091,
      "step": 3100
    },
    {
      "epoch": 0.627142569066344,
      "grad_norm": 2.6430556774139404,
      "learning_rate": 3.432647711232103e-05,
      "loss": 0.5029,
      "step": 3110
    },
    {
      "epoch": 0.6291591046581972,
      "grad_norm": 2.37449049949646,
      "learning_rate": 3.42760637225247e-05,
      "loss": 0.418,
      "step": 3120
    },
    {
      "epoch": 0.6311756402500505,
      "grad_norm": 19.311613082885742,
      "learning_rate": 3.4225650332728375e-05,
      "loss": 0.5863,
      "step": 3130
    },
    {
      "epoch": 0.6331921758419036,
      "grad_norm": 1.6177524328231812,
      "learning_rate": 3.417523694293204e-05,
      "loss": 0.5447,
      "step": 3140
    },
    {
      "epoch": 0.6352087114337568,
      "grad_norm": 1.8636491298675537,
      "learning_rate": 3.4124823553135715e-05,
      "loss": 0.3483,
      "step": 3150
    },
    {
      "epoch": 0.63722524702561,
      "grad_norm": 12.154297828674316,
      "learning_rate": 3.407441016333938e-05,
      "loss": 0.4721,
      "step": 3160
    },
    {
      "epoch": 0.6392417826174632,
      "grad_norm": 6.363089561462402,
      "learning_rate": 3.4023996773543055e-05,
      "loss": 0.3712,
      "step": 3170
    },
    {
      "epoch": 0.6412583182093164,
      "grad_norm": 10.393641471862793,
      "learning_rate": 3.397358338374673e-05,
      "loss": 0.235,
      "step": 3180
    },
    {
      "epoch": 0.6432748538011696,
      "grad_norm": 2.0399932861328125,
      "learning_rate": 3.3923169993950394e-05,
      "loss": 0.3821,
      "step": 3190
    },
    {
      "epoch": 0.6452913893930228,
      "grad_norm": 30.656570434570312,
      "learning_rate": 3.387275660415407e-05,
      "loss": 0.4347,
      "step": 3200
    },
    {
      "epoch": 0.647307924984876,
      "grad_norm": 1.3885586261749268,
      "learning_rate": 3.3822343214357734e-05,
      "loss": 0.5509,
      "step": 3210
    },
    {
      "epoch": 0.6493244605767292,
      "grad_norm": 1.7105787992477417,
      "learning_rate": 3.377192982456141e-05,
      "loss": 0.5163,
      "step": 3220
    },
    {
      "epoch": 0.6513409961685823,
      "grad_norm": 1.7007349729537964,
      "learning_rate": 3.3721516434765074e-05,
      "loss": 0.3683,
      "step": 3230
    },
    {
      "epoch": 0.6533575317604355,
      "grad_norm": 1.0099507570266724,
      "learning_rate": 3.367110304496874e-05,
      "loss": 0.273,
      "step": 3240
    },
    {
      "epoch": 0.6553740673522888,
      "grad_norm": 12.710569381713867,
      "learning_rate": 3.3620689655172414e-05,
      "loss": 0.3519,
      "step": 3250
    },
    {
      "epoch": 0.657390602944142,
      "grad_norm": 2.1935782432556152,
      "learning_rate": 3.357027626537608e-05,
      "loss": 0.401,
      "step": 3260
    },
    {
      "epoch": 0.6594071385359952,
      "grad_norm": 42.258155822753906,
      "learning_rate": 3.351986287557975e-05,
      "loss": 0.4099,
      "step": 3270
    },
    {
      "epoch": 0.6614236741278483,
      "grad_norm": 3.593461513519287,
      "learning_rate": 3.346944948578343e-05,
      "loss": 0.2445,
      "step": 3280
    },
    {
      "epoch": 0.6634402097197015,
      "grad_norm": 1.379526972770691,
      "learning_rate": 3.341903609598709e-05,
      "loss": 0.5455,
      "step": 3290
    },
    {
      "epoch": 0.6654567453115547,
      "grad_norm": 2.735232353210449,
      "learning_rate": 3.3368622706190766e-05,
      "loss": 0.4301,
      "step": 3300
    },
    {
      "epoch": 0.667473280903408,
      "grad_norm": 2.399245500564575,
      "learning_rate": 3.331820931639444e-05,
      "loss": 0.5408,
      "step": 3310
    },
    {
      "epoch": 0.6694898164952612,
      "grad_norm": 94.6304702758789,
      "learning_rate": 3.3267795926598106e-05,
      "loss": 0.3846,
      "step": 3320
    },
    {
      "epoch": 0.6715063520871143,
      "grad_norm": 1.0916473865509033,
      "learning_rate": 3.321738253680178e-05,
      "loss": 0.4411,
      "step": 3330
    },
    {
      "epoch": 0.6735228876789675,
      "grad_norm": 17.212385177612305,
      "learning_rate": 3.3166969147005446e-05,
      "loss": 0.3607,
      "step": 3340
    },
    {
      "epoch": 0.6755394232708207,
      "grad_norm": 15.274550437927246,
      "learning_rate": 3.311655575720912e-05,
      "loss": 0.3768,
      "step": 3350
    },
    {
      "epoch": 0.677555958862674,
      "grad_norm": 1.4460645914077759,
      "learning_rate": 3.306614236741279e-05,
      "loss": 0.3542,
      "step": 3360
    },
    {
      "epoch": 0.6795724944545272,
      "grad_norm": 79.9557876586914,
      "learning_rate": 3.301572897761646e-05,
      "loss": 0.4903,
      "step": 3370
    },
    {
      "epoch": 0.6815890300463803,
      "grad_norm": 2.185232400894165,
      "learning_rate": 3.2965315587820125e-05,
      "loss": 0.5154,
      "step": 3380
    },
    {
      "epoch": 0.6836055656382335,
      "grad_norm": 1.9688677787780762,
      "learning_rate": 3.291490219802379e-05,
      "loss": 0.4287,
      "step": 3390
    },
    {
      "epoch": 0.6856221012300867,
      "grad_norm": 0.7297257781028748,
      "learning_rate": 3.2864488808227465e-05,
      "loss": 0.3355,
      "step": 3400
    },
    {
      "epoch": 0.6876386368219399,
      "grad_norm": 3.360184907913208,
      "learning_rate": 3.281407541843114e-05,
      "loss": 0.3881,
      "step": 3410
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 7.004738807678223,
      "learning_rate": 3.2763662028634805e-05,
      "loss": 0.4504,
      "step": 3420
    },
    {
      "epoch": 0.6916717080056463,
      "grad_norm": 16.89578628540039,
      "learning_rate": 3.271324863883848e-05,
      "loss": 0.4752,
      "step": 3430
    },
    {
      "epoch": 0.6936882435974995,
      "grad_norm": 1.5407166481018066,
      "learning_rate": 3.2662835249042144e-05,
      "loss": 0.3932,
      "step": 3440
    },
    {
      "epoch": 0.6957047791893527,
      "grad_norm": 17.564586639404297,
      "learning_rate": 3.261242185924582e-05,
      "loss": 0.3868,
      "step": 3450
    },
    {
      "epoch": 0.6977213147812059,
      "grad_norm": 6.171798229217529,
      "learning_rate": 3.256200846944949e-05,
      "loss": 0.3496,
      "step": 3460
    },
    {
      "epoch": 0.6997378503730591,
      "grad_norm": 34.24599075317383,
      "learning_rate": 3.251159507965316e-05,
      "loss": 0.7854,
      "step": 3470
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 63.37495803833008,
      "learning_rate": 3.246118168985683e-05,
      "loss": 0.4677,
      "step": 3480
    },
    {
      "epoch": 0.7037709215567655,
      "grad_norm": 1.2060595750808716,
      "learning_rate": 3.24107683000605e-05,
      "loss": 0.3022,
      "step": 3490
    },
    {
      "epoch": 0.7057874571486187,
      "grad_norm": 55.446327209472656,
      "learning_rate": 3.236035491026417e-05,
      "loss": 0.3439,
      "step": 3500
    },
    {
      "epoch": 0.7078039927404719,
      "grad_norm": 20.035430908203125,
      "learning_rate": 3.230994152046784e-05,
      "loss": 0.3493,
      "step": 3510
    },
    {
      "epoch": 0.7098205283323251,
      "grad_norm": 5.2553558349609375,
      "learning_rate": 3.225952813067151e-05,
      "loss": 0.2617,
      "step": 3520
    },
    {
      "epoch": 0.7118370639241782,
      "grad_norm": 1.5942171812057495,
      "learning_rate": 3.220911474087518e-05,
      "loss": 0.525,
      "step": 3530
    },
    {
      "epoch": 0.7138535995160314,
      "grad_norm": 36.698081970214844,
      "learning_rate": 3.215870135107884e-05,
      "loss": 0.3593,
      "step": 3540
    },
    {
      "epoch": 0.7158701351078847,
      "grad_norm": 3.9449713230133057,
      "learning_rate": 3.2108287961282516e-05,
      "loss": 0.5411,
      "step": 3550
    },
    {
      "epoch": 0.7178866706997379,
      "grad_norm": 44.67990493774414,
      "learning_rate": 3.205787457148619e-05,
      "loss": 0.5157,
      "step": 3560
    },
    {
      "epoch": 0.719903206291591,
      "grad_norm": 14.777212142944336,
      "learning_rate": 3.2007461181689856e-05,
      "loss": 0.4647,
      "step": 3570
    },
    {
      "epoch": 0.7219197418834442,
      "grad_norm": 3.397749423980713,
      "learning_rate": 3.195704779189353e-05,
      "loss": 0.4183,
      "step": 3580
    },
    {
      "epoch": 0.7239362774752974,
      "grad_norm": 1.4909805059432983,
      "learning_rate": 3.1906634402097196e-05,
      "loss": 0.3408,
      "step": 3590
    },
    {
      "epoch": 0.7259528130671506,
      "grad_norm": 2.525089979171753,
      "learning_rate": 3.185622101230087e-05,
      "loss": 0.4461,
      "step": 3600
    },
    {
      "epoch": 0.7279693486590039,
      "grad_norm": 15.950508117675781,
      "learning_rate": 3.180580762250454e-05,
      "loss": 0.3821,
      "step": 3610
    },
    {
      "epoch": 0.729985884250857,
      "grad_norm": 8.339688301086426,
      "learning_rate": 3.175539423270821e-05,
      "loss": 0.4436,
      "step": 3620
    },
    {
      "epoch": 0.7320024198427102,
      "grad_norm": 4.065456867218018,
      "learning_rate": 3.170498084291188e-05,
      "loss": 0.3851,
      "step": 3630
    },
    {
      "epoch": 0.7340189554345634,
      "grad_norm": 3.0345778465270996,
      "learning_rate": 3.165456745311555e-05,
      "loss": 0.3592,
      "step": 3640
    },
    {
      "epoch": 0.7360354910264166,
      "grad_norm": 1.4667459726333618,
      "learning_rate": 3.160415406331922e-05,
      "loss": 0.3976,
      "step": 3650
    },
    {
      "epoch": 0.7380520266182699,
      "grad_norm": 2.351166248321533,
      "learning_rate": 3.155374067352289e-05,
      "loss": 0.3398,
      "step": 3660
    },
    {
      "epoch": 0.740068562210123,
      "grad_norm": 41.573307037353516,
      "learning_rate": 3.1503327283726555e-05,
      "loss": 0.3527,
      "step": 3670
    },
    {
      "epoch": 0.7420850978019762,
      "grad_norm": 7.566044807434082,
      "learning_rate": 3.145291389393023e-05,
      "loss": 0.604,
      "step": 3680
    },
    {
      "epoch": 0.7441016333938294,
      "grad_norm": 41.05691909790039,
      "learning_rate": 3.1402500504133894e-05,
      "loss": 0.5142,
      "step": 3690
    },
    {
      "epoch": 0.7461181689856826,
      "grad_norm": 8.405415534973145,
      "learning_rate": 3.135208711433757e-05,
      "loss": 0.3291,
      "step": 3700
    },
    {
      "epoch": 0.7481347045775358,
      "grad_norm": 17.508304595947266,
      "learning_rate": 3.130167372454124e-05,
      "loss": 0.3276,
      "step": 3710
    },
    {
      "epoch": 0.7501512401693889,
      "grad_norm": 2.4077653884887695,
      "learning_rate": 3.125126033474491e-05,
      "loss": 0.2942,
      "step": 3720
    },
    {
      "epoch": 0.7521677757612422,
      "grad_norm": 1.5414544343948364,
      "learning_rate": 3.120084694494858e-05,
      "loss": 0.49,
      "step": 3730
    },
    {
      "epoch": 0.7541843113530954,
      "grad_norm": 7.177304267883301,
      "learning_rate": 3.115043355515225e-05,
      "loss": 0.3837,
      "step": 3740
    },
    {
      "epoch": 0.7562008469449486,
      "grad_norm": 20.148324966430664,
      "learning_rate": 3.110002016535592e-05,
      "loss": 0.6364,
      "step": 3750
    },
    {
      "epoch": 0.7582173825368018,
      "grad_norm": 11.547067642211914,
      "learning_rate": 3.1049606775559594e-05,
      "loss": 0.3835,
      "step": 3760
    },
    {
      "epoch": 0.7602339181286549,
      "grad_norm": 1.122170090675354,
      "learning_rate": 3.099919338576326e-05,
      "loss": 0.4615,
      "step": 3770
    },
    {
      "epoch": 0.7622504537205081,
      "grad_norm": 4.000256538391113,
      "learning_rate": 3.0948779995966933e-05,
      "loss": 0.3223,
      "step": 3780
    },
    {
      "epoch": 0.7642669893123614,
      "grad_norm": 4.218289852142334,
      "learning_rate": 3.08983666061706e-05,
      "loss": 0.4239,
      "step": 3790
    },
    {
      "epoch": 0.7662835249042146,
      "grad_norm": 2.2481048107147217,
      "learning_rate": 3.084795321637427e-05,
      "loss": 0.4952,
      "step": 3800
    },
    {
      "epoch": 0.7683000604960678,
      "grad_norm": 21.837682723999023,
      "learning_rate": 3.079753982657794e-05,
      "loss": 0.3234,
      "step": 3810
    },
    {
      "epoch": 0.7703165960879209,
      "grad_norm": 1.9931092262268066,
      "learning_rate": 3.0747126436781606e-05,
      "loss": 0.3958,
      "step": 3820
    },
    {
      "epoch": 0.7723331316797741,
      "grad_norm": 21.098892211914062,
      "learning_rate": 3.069671304698528e-05,
      "loss": 0.2529,
      "step": 3830
    },
    {
      "epoch": 0.7743496672716274,
      "grad_norm": 5.286548614501953,
      "learning_rate": 3.0646299657188946e-05,
      "loss": 0.3327,
      "step": 3840
    },
    {
      "epoch": 0.7763662028634806,
      "grad_norm": 16.509502410888672,
      "learning_rate": 3.059588626739262e-05,
      "loss": 0.5478,
      "step": 3850
    },
    {
      "epoch": 0.7783827384553338,
      "grad_norm": 79.84529113769531,
      "learning_rate": 3.054547287759629e-05,
      "loss": 0.5072,
      "step": 3860
    },
    {
      "epoch": 0.7803992740471869,
      "grad_norm": 72.5807113647461,
      "learning_rate": 3.049505948779996e-05,
      "loss": 0.2909,
      "step": 3870
    },
    {
      "epoch": 0.7824158096390401,
      "grad_norm": 56.69296646118164,
      "learning_rate": 3.0444646098003632e-05,
      "loss": 0.3432,
      "step": 3880
    },
    {
      "epoch": 0.7844323452308933,
      "grad_norm": 11.137812614440918,
      "learning_rate": 3.03942327082073e-05,
      "loss": 0.3124,
      "step": 3890
    },
    {
      "epoch": 0.7864488808227466,
      "grad_norm": 9.679248809814453,
      "learning_rate": 3.0343819318410972e-05,
      "loss": 0.4431,
      "step": 3900
    },
    {
      "epoch": 0.7884654164145997,
      "grad_norm": 40.30107498168945,
      "learning_rate": 3.029340592861464e-05,
      "loss": 0.4265,
      "step": 3910
    },
    {
      "epoch": 0.7904819520064529,
      "grad_norm": 9.52298641204834,
      "learning_rate": 3.024299253881831e-05,
      "loss": 0.451,
      "step": 3920
    },
    {
      "epoch": 0.7924984875983061,
      "grad_norm": 2.199306011199951,
      "learning_rate": 3.019257914902198e-05,
      "loss": 0.6213,
      "step": 3930
    },
    {
      "epoch": 0.7945150231901593,
      "grad_norm": 10.936162948608398,
      "learning_rate": 3.0142165759225648e-05,
      "loss": 0.3692,
      "step": 3940
    },
    {
      "epoch": 0.7965315587820125,
      "grad_norm": 2.4107847213745117,
      "learning_rate": 3.009175236942932e-05,
      "loss": 0.3747,
      "step": 3950
    },
    {
      "epoch": 0.7985480943738656,
      "grad_norm": 1.1782046556472778,
      "learning_rate": 3.0041338979632994e-05,
      "loss": 0.3237,
      "step": 3960
    },
    {
      "epoch": 0.8005646299657189,
      "grad_norm": 23.088356018066406,
      "learning_rate": 2.999092558983666e-05,
      "loss": 0.4397,
      "step": 3970
    },
    {
      "epoch": 0.8025811655575721,
      "grad_norm": 1.8625996112823486,
      "learning_rate": 2.9940512200040334e-05,
      "loss": 0.4177,
      "step": 3980
    },
    {
      "epoch": 0.8045977011494253,
      "grad_norm": 20.47756004333496,
      "learning_rate": 2.9890098810244004e-05,
      "loss": 0.4583,
      "step": 3990
    },
    {
      "epoch": 0.8066142367412785,
      "grad_norm": 1.4917628765106201,
      "learning_rate": 2.983968542044767e-05,
      "loss": 0.4175,
      "step": 4000
    },
    {
      "epoch": 0.8086307723331316,
      "grad_norm": 9.270242691040039,
      "learning_rate": 2.9789272030651344e-05,
      "loss": 0.5008,
      "step": 4010
    },
    {
      "epoch": 0.8106473079249849,
      "grad_norm": 2.2230188846588135,
      "learning_rate": 2.973885864085501e-05,
      "loss": 0.3779,
      "step": 4020
    },
    {
      "epoch": 0.8126638435168381,
      "grad_norm": 1.8047722578048706,
      "learning_rate": 2.9688445251058683e-05,
      "loss": 0.4024,
      "step": 4030
    },
    {
      "epoch": 0.8146803791086913,
      "grad_norm": 3.2126317024230957,
      "learning_rate": 2.9638031861262357e-05,
      "loss": 0.2477,
      "step": 4040
    },
    {
      "epoch": 0.8166969147005445,
      "grad_norm": 3.2873899936676025,
      "learning_rate": 2.9587618471466023e-05,
      "loss": 0.4157,
      "step": 4050
    },
    {
      "epoch": 0.8187134502923976,
      "grad_norm": 4.268347263336182,
      "learning_rate": 2.9537205081669693e-05,
      "loss": 0.4414,
      "step": 4060
    },
    {
      "epoch": 0.8207299858842508,
      "grad_norm": 12.831602096557617,
      "learning_rate": 2.948679169187336e-05,
      "loss": 0.4818,
      "step": 4070
    },
    {
      "epoch": 0.822746521476104,
      "grad_norm": 2.3479549884796143,
      "learning_rate": 2.9436378302077033e-05,
      "loss": 0.3331,
      "step": 4080
    },
    {
      "epoch": 0.8247630570679573,
      "grad_norm": 7.8531670570373535,
      "learning_rate": 2.9385964912280706e-05,
      "loss": 0.385,
      "step": 4090
    },
    {
      "epoch": 0.8267795926598105,
      "grad_norm": 1.4033085107803345,
      "learning_rate": 2.9335551522484372e-05,
      "loss": 0.2961,
      "step": 4100
    },
    {
      "epoch": 0.8287961282516636,
      "grad_norm": 1.9787694215774536,
      "learning_rate": 2.9285138132688046e-05,
      "loss": 0.3346,
      "step": 4110
    },
    {
      "epoch": 0.8308126638435168,
      "grad_norm": 1.3026220798492432,
      "learning_rate": 2.9234724742891712e-05,
      "loss": 0.2741,
      "step": 4120
    },
    {
      "epoch": 0.83282919943537,
      "grad_norm": 1.3435832262039185,
      "learning_rate": 2.9184311353095382e-05,
      "loss": 0.3083,
      "step": 4130
    },
    {
      "epoch": 0.8348457350272233,
      "grad_norm": 13.994401931762695,
      "learning_rate": 2.9133897963299055e-05,
      "loss": 0.3991,
      "step": 4140
    },
    {
      "epoch": 0.8368622706190765,
      "grad_norm": 5.123234748840332,
      "learning_rate": 2.9083484573502722e-05,
      "loss": 0.4253,
      "step": 4150
    },
    {
      "epoch": 0.8388788062109296,
      "grad_norm": 2.3133037090301514,
      "learning_rate": 2.9033071183706395e-05,
      "loss": 0.5331,
      "step": 4160
    },
    {
      "epoch": 0.8408953418027828,
      "grad_norm": 3.9698431491851807,
      "learning_rate": 2.898265779391006e-05,
      "loss": 0.2206,
      "step": 4170
    },
    {
      "epoch": 0.842911877394636,
      "grad_norm": 8.853059768676758,
      "learning_rate": 2.8932244404113735e-05,
      "loss": 0.6301,
      "step": 4180
    },
    {
      "epoch": 0.8449284129864892,
      "grad_norm": 2.9736456871032715,
      "learning_rate": 2.8881831014317408e-05,
      "loss": 0.3988,
      "step": 4190
    },
    {
      "epoch": 0.8469449485783425,
      "grad_norm": 9.425455093383789,
      "learning_rate": 2.8831417624521075e-05,
      "loss": 0.3905,
      "step": 4200
    },
    {
      "epoch": 0.8489614841701956,
      "grad_norm": 1.4506986141204834,
      "learning_rate": 2.8781004234724744e-05,
      "loss": 0.2585,
      "step": 4210
    },
    {
      "epoch": 0.8509780197620488,
      "grad_norm": 9.198850631713867,
      "learning_rate": 2.873059084492841e-05,
      "loss": 0.3687,
      "step": 4220
    },
    {
      "epoch": 0.852994555353902,
      "grad_norm": 14.52780532836914,
      "learning_rate": 2.8680177455132084e-05,
      "loss": 0.455,
      "step": 4230
    },
    {
      "epoch": 0.8550110909457552,
      "grad_norm": 2.1668128967285156,
      "learning_rate": 2.8629764065335757e-05,
      "loss": 0.3945,
      "step": 4240
    },
    {
      "epoch": 0.8570276265376083,
      "grad_norm": 6.11374044418335,
      "learning_rate": 2.8579350675539424e-05,
      "loss": 0.4899,
      "step": 4250
    },
    {
      "epoch": 0.8590441621294616,
      "grad_norm": 2.441507577896118,
      "learning_rate": 2.8528937285743097e-05,
      "loss": 0.3703,
      "step": 4260
    },
    {
      "epoch": 0.8610606977213148,
      "grad_norm": 1.6464529037475586,
      "learning_rate": 2.8478523895946764e-05,
      "loss": 0.3703,
      "step": 4270
    },
    {
      "epoch": 0.863077233313168,
      "grad_norm": 1.1298140287399292,
      "learning_rate": 2.8428110506150433e-05,
      "loss": 0.4171,
      "step": 4280
    },
    {
      "epoch": 0.8650937689050212,
      "grad_norm": 11.370234489440918,
      "learning_rate": 2.8377697116354107e-05,
      "loss": 0.3956,
      "step": 4290
    },
    {
      "epoch": 0.8671103044968743,
      "grad_norm": 3.776796579360962,
      "learning_rate": 2.8327283726557773e-05,
      "loss": 0.3875,
      "step": 4300
    },
    {
      "epoch": 0.8691268400887275,
      "grad_norm": 1.9049439430236816,
      "learning_rate": 2.8276870336761446e-05,
      "loss": 0.5262,
      "step": 4310
    },
    {
      "epoch": 0.8711433756805808,
      "grad_norm": 3.7625925540924072,
      "learning_rate": 2.8226456946965113e-05,
      "loss": 0.4052,
      "step": 4320
    },
    {
      "epoch": 0.873159911272434,
      "grad_norm": 1.2838000059127808,
      "learning_rate": 2.8176043557168786e-05,
      "loss": 0.3592,
      "step": 4330
    },
    {
      "epoch": 0.8751764468642872,
      "grad_norm": 19.944725036621094,
      "learning_rate": 2.8125630167372456e-05,
      "loss": 0.3375,
      "step": 4340
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 10.088780403137207,
      "learning_rate": 2.8075216777576126e-05,
      "loss": 0.503,
      "step": 4350
    },
    {
      "epoch": 0.8792095180479935,
      "grad_norm": 4.497416973114014,
      "learning_rate": 2.8024803387779796e-05,
      "loss": 0.3746,
      "step": 4360
    },
    {
      "epoch": 0.8812260536398467,
      "grad_norm": 1.4661599397659302,
      "learning_rate": 2.7974389997983462e-05,
      "loss": 0.3887,
      "step": 4370
    },
    {
      "epoch": 0.8832425892317,
      "grad_norm": 43.94038772583008,
      "learning_rate": 2.7923976608187135e-05,
      "loss": 0.5104,
      "step": 4380
    },
    {
      "epoch": 0.8852591248235532,
      "grad_norm": 4.581221580505371,
      "learning_rate": 2.787356321839081e-05,
      "loss": 0.4587,
      "step": 4390
    },
    {
      "epoch": 0.8872756604154063,
      "grad_norm": 0.8618960976600647,
      "learning_rate": 2.7823149828594475e-05,
      "loss": 0.5352,
      "step": 4400
    },
    {
      "epoch": 0.8892921960072595,
      "grad_norm": 8.428305625915527,
      "learning_rate": 2.777273643879815e-05,
      "loss": 0.3958,
      "step": 4410
    },
    {
      "epoch": 0.8913087315991127,
      "grad_norm": 1.0254237651824951,
      "learning_rate": 2.7722323049001815e-05,
      "loss": 0.3939,
      "step": 4420
    },
    {
      "epoch": 0.8933252671909659,
      "grad_norm": 2.1114864349365234,
      "learning_rate": 2.7671909659205485e-05,
      "loss": 0.431,
      "step": 4430
    },
    {
      "epoch": 0.8953418027828192,
      "grad_norm": 18.410612106323242,
      "learning_rate": 2.7621496269409158e-05,
      "loss": 0.4612,
      "step": 4440
    },
    {
      "epoch": 0.8973583383746723,
      "grad_norm": 50.5177001953125,
      "learning_rate": 2.7571082879612825e-05,
      "loss": 0.3306,
      "step": 4450
    },
    {
      "epoch": 0.8993748739665255,
      "grad_norm": 33.30152130126953,
      "learning_rate": 2.7520669489816498e-05,
      "loss": 0.4128,
      "step": 4460
    },
    {
      "epoch": 0.9013914095583787,
      "grad_norm": 3.3255350589752197,
      "learning_rate": 2.7470256100020164e-05,
      "loss": 0.3971,
      "step": 4470
    },
    {
      "epoch": 0.9034079451502319,
      "grad_norm": 1.2428134679794312,
      "learning_rate": 2.7419842710223838e-05,
      "loss": 0.3302,
      "step": 4480
    },
    {
      "epoch": 0.9054244807420851,
      "grad_norm": 0.9239177703857422,
      "learning_rate": 2.7369429320427507e-05,
      "loss": 0.3811,
      "step": 4490
    },
    {
      "epoch": 0.9074410163339383,
      "grad_norm": 1.010628342628479,
      "learning_rate": 2.7319015930631174e-05,
      "loss": 0.3804,
      "step": 4500
    },
    {
      "epoch": 0.9094575519257915,
      "grad_norm": 5.448174953460693,
      "learning_rate": 2.7268602540834847e-05,
      "loss": 0.3687,
      "step": 4510
    },
    {
      "epoch": 0.9114740875176447,
      "grad_norm": 36.01120376586914,
      "learning_rate": 2.7218189151038514e-05,
      "loss": 0.3043,
      "step": 4520
    },
    {
      "epoch": 0.9134906231094979,
      "grad_norm": 43.696495056152344,
      "learning_rate": 2.7167775761242187e-05,
      "loss": 0.3665,
      "step": 4530
    },
    {
      "epoch": 0.9155071587013511,
      "grad_norm": 12.40044116973877,
      "learning_rate": 2.711736237144586e-05,
      "loss": 0.325,
      "step": 4540
    },
    {
      "epoch": 0.9175236942932042,
      "grad_norm": 22.231863021850586,
      "learning_rate": 2.7066948981649527e-05,
      "loss": 0.3464,
      "step": 4550
    },
    {
      "epoch": 0.9195402298850575,
      "grad_norm": 6.6040472984313965,
      "learning_rate": 2.70165355918532e-05,
      "loss": 0.3773,
      "step": 4560
    },
    {
      "epoch": 0.9215567654769107,
      "grad_norm": 6.148664474487305,
      "learning_rate": 2.6966122202056866e-05,
      "loss": 0.3659,
      "step": 4570
    },
    {
      "epoch": 0.9235733010687639,
      "grad_norm": 3.721264362335205,
      "learning_rate": 2.6915708812260536e-05,
      "loss": 0.5939,
      "step": 4580
    },
    {
      "epoch": 0.925589836660617,
      "grad_norm": 13.008739471435547,
      "learning_rate": 2.686529542246421e-05,
      "loss": 0.3432,
      "step": 4590
    },
    {
      "epoch": 0.9276063722524702,
      "grad_norm": 23.82500457763672,
      "learning_rate": 2.6814882032667876e-05,
      "loss": 0.4741,
      "step": 4600
    },
    {
      "epoch": 0.9296229078443234,
      "grad_norm": 141.23651123046875,
      "learning_rate": 2.676446864287155e-05,
      "loss": 0.5345,
      "step": 4610
    },
    {
      "epoch": 0.9316394434361767,
      "grad_norm": 1.1648499965667725,
      "learning_rate": 2.6714055253075216e-05,
      "loss": 0.2627,
      "step": 4620
    },
    {
      "epoch": 0.9336559790280299,
      "grad_norm": 6.88178014755249,
      "learning_rate": 2.666364186327889e-05,
      "loss": 0.2639,
      "step": 4630
    },
    {
      "epoch": 0.935672514619883,
      "grad_norm": 3.0843024253845215,
      "learning_rate": 2.661322847348256e-05,
      "loss": 0.7079,
      "step": 4640
    },
    {
      "epoch": 0.9376890502117362,
      "grad_norm": 9.449329376220703,
      "learning_rate": 2.6562815083686225e-05,
      "loss": 0.2991,
      "step": 4650
    },
    {
      "epoch": 0.9397055858035894,
      "grad_norm": 492.8753662109375,
      "learning_rate": 2.65124016938899e-05,
      "loss": 0.4022,
      "step": 4660
    },
    {
      "epoch": 0.9417221213954426,
      "grad_norm": 14.758708000183105,
      "learning_rate": 2.6461988304093572e-05,
      "loss": 0.5419,
      "step": 4670
    },
    {
      "epoch": 0.9437386569872959,
      "grad_norm": 3.836827039718628,
      "learning_rate": 2.6411574914297238e-05,
      "loss": 0.3227,
      "step": 4680
    },
    {
      "epoch": 0.945755192579149,
      "grad_norm": 4.4470744132995605,
      "learning_rate": 2.636116152450091e-05,
      "loss": 0.4597,
      "step": 4690
    },
    {
      "epoch": 0.9477717281710022,
      "grad_norm": 67.58587646484375,
      "learning_rate": 2.6310748134704578e-05,
      "loss": 0.5096,
      "step": 4700
    },
    {
      "epoch": 0.9497882637628554,
      "grad_norm": 8.982855796813965,
      "learning_rate": 2.6260334744908248e-05,
      "loss": 0.3454,
      "step": 4710
    },
    {
      "epoch": 0.9518047993547086,
      "grad_norm": 1.546796202659607,
      "learning_rate": 2.620992135511192e-05,
      "loss": 0.3368,
      "step": 4720
    },
    {
      "epoch": 0.9538213349465618,
      "grad_norm": 6.241573333740234,
      "learning_rate": 2.6159507965315588e-05,
      "loss": 0.3118,
      "step": 4730
    },
    {
      "epoch": 0.955837870538415,
      "grad_norm": 3.8422064781188965,
      "learning_rate": 2.610909457551926e-05,
      "loss": 0.2897,
      "step": 4740
    },
    {
      "epoch": 0.9578544061302682,
      "grad_norm": 4.924294471740723,
      "learning_rate": 2.6058681185722927e-05,
      "loss": 0.3204,
      "step": 4750
    },
    {
      "epoch": 0.9598709417221214,
      "grad_norm": 8.079270362854004,
      "learning_rate": 2.60082677959266e-05,
      "loss": 0.394,
      "step": 4760
    },
    {
      "epoch": 0.9618874773139746,
      "grad_norm": 28.95589256286621,
      "learning_rate": 2.595785440613027e-05,
      "loss": 0.4566,
      "step": 4770
    },
    {
      "epoch": 0.9639040129058278,
      "grad_norm": 2.60673189163208,
      "learning_rate": 2.590744101633394e-05,
      "loss": 0.4204,
      "step": 4780
    },
    {
      "epoch": 0.9659205484976809,
      "grad_norm": 29.111398696899414,
      "learning_rate": 2.585702762653761e-05,
      "loss": 0.3563,
      "step": 4790
    },
    {
      "epoch": 0.9679370840895342,
      "grad_norm": 16.976490020751953,
      "learning_rate": 2.5806614236741277e-05,
      "loss": 0.2883,
      "step": 4800
    },
    {
      "epoch": 0.9699536196813874,
      "grad_norm": 3.9103331565856934,
      "learning_rate": 2.575620084694495e-05,
      "loss": 0.3624,
      "step": 4810
    },
    {
      "epoch": 0.9719701552732406,
      "grad_norm": 29.711332321166992,
      "learning_rate": 2.5705787457148623e-05,
      "loss": 0.5197,
      "step": 4820
    },
    {
      "epoch": 0.9739866908650938,
      "grad_norm": 8.089860916137695,
      "learning_rate": 2.565537406735229e-05,
      "loss": 0.4031,
      "step": 4830
    },
    {
      "epoch": 0.9760032264569469,
      "grad_norm": 5.372949123382568,
      "learning_rate": 2.5604960677555963e-05,
      "loss": 0.4768,
      "step": 4840
    },
    {
      "epoch": 0.9780197620488001,
      "grad_norm": 31.09061050415039,
      "learning_rate": 2.555454728775963e-05,
      "loss": 0.4562,
      "step": 4850
    },
    {
      "epoch": 0.9800362976406534,
      "grad_norm": 1.0550932884216309,
      "learning_rate": 2.55041338979633e-05,
      "loss": 0.3333,
      "step": 4860
    },
    {
      "epoch": 0.9820528332325066,
      "grad_norm": 14.580116271972656,
      "learning_rate": 2.5453720508166972e-05,
      "loss": 0.3934,
      "step": 4870
    },
    {
      "epoch": 0.9840693688243598,
      "grad_norm": 7.829423904418945,
      "learning_rate": 2.540330711837064e-05,
      "loss": 0.4163,
      "step": 4880
    },
    {
      "epoch": 0.9860859044162129,
      "grad_norm": 1.585466742515564,
      "learning_rate": 2.5352893728574312e-05,
      "loss": 0.3756,
      "step": 4890
    },
    {
      "epoch": 0.9881024400080661,
      "grad_norm": 39.143638610839844,
      "learning_rate": 2.530248033877798e-05,
      "loss": 0.4343,
      "step": 4900
    },
    {
      "epoch": 0.9901189755999193,
      "grad_norm": 1.165976643562317,
      "learning_rate": 2.5252066948981652e-05,
      "loss": 0.4984,
      "step": 4910
    },
    {
      "epoch": 0.9921355111917726,
      "grad_norm": 0.9850594401359558,
      "learning_rate": 2.5201653559185322e-05,
      "loss": 0.3976,
      "step": 4920
    },
    {
      "epoch": 0.9941520467836257,
      "grad_norm": 21.779722213745117,
      "learning_rate": 2.5151240169388988e-05,
      "loss": 0.4816,
      "step": 4930
    },
    {
      "epoch": 0.9961685823754789,
      "grad_norm": 2.683067560195923,
      "learning_rate": 2.510082677959266e-05,
      "loss": 0.5152,
      "step": 4940
    },
    {
      "epoch": 0.9981851179673321,
      "grad_norm": 1.0628340244293213,
      "learning_rate": 2.5050413389796328e-05,
      "loss": 0.4292,
      "step": 4950
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.42764294147491455,
      "eval_runtime": 590.9975,
      "eval_samples_per_second": 16.78,
      "eval_steps_per_second": 2.098,
      "step": 4959
    }
  ],
  "logging_steps": 10,
  "max_steps": 9918,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2609075002713600.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
